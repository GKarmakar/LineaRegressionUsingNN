{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras a warpper API that runs on top of Tensorflow or theano is very popular and easy to use. Scikit learn also very popular libraries for machine learning.In this post I will show how to use keras and scikit learn to build neural network architecture in python and develop a regression linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(sklearn.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Keras APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.constraints import maxnorm\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we will solve here is to predict the peer rank of mutual funds based on the funs characteristics.\n",
    "We will use train.csv file that has fund characteristics and peerrank for 3 separate periods and 24 different rank categories for each fund.\n",
    "Our objective is to train a regression model in keras to be able to predict peer rank for new funds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def columnNameBuilder(columnName, numberOfItems):\n",
    "    columns = [ columnName + ' ' + str(i) for i in range(numberOfItems)]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit learn works with numerical data, so we need to transform our categorical non-numeric data into numerical representation. we will create a function to engineer certain features and drop columns that are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineer(dataset):\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    prospectus_objective = dataset[\"Prospectus Objective\"].values\n",
    "    prospectus_objective = le.fit_transform(prospectus_objective)\n",
    "    dataset[\"Prospectus Objective\"] = prospectus_objective\n",
    "    \n",
    "    col_names = columnNameBuilder(\"Average Credit Quality\", 36)\n",
    "    for i in range(36):\n",
    "        col_name = col_names[i]\n",
    "        dataset[col_name].fillna('0', inplace=True)\n",
    "        col_val = dataset[col_name].values\n",
    "        dataset[col_name] = le.fit_transform(col_val)\n",
    "            \n",
    "    dataset.loc[dataset['Available In Insurance Product'] == 'Yes', 'Available In Insurance Product'] = 1\n",
    "    dataset.loc[dataset['Available In Insurance Product'] == 'No', 'Available In Insurance Product'] = 0\n",
    "    dataset.loc[dataset['Available For Retirement Plan'] == 'Yes', 'Available For Retirement Plan'] = 1\n",
    "    dataset.loc[dataset['Available For Retirement Plan'] == 'No', 'Available For Retirement Plan'] = 0\n",
    "    dataset[\"Available In Insurance Product\"] = dataset[\"Available In Insurance Product\"].astype(float)\n",
    "    dataset[\"Available For Retirement Plan\"] = dataset[\"Available For Retirement Plan\"].astype(float)\n",
    "\n",
    "    dataset.drop(\"Primary Prospectus Benchmark\", inplace = True, axis=1)\n",
    "    dataset.drop(\"Firm Name\", inplace = True, axis=1)\n",
    "    dataset.drop('.id', inplace = True, axis=1)\n",
    "    dataset.drop(\"Name\", inplace = True, axis=1)\n",
    "    dataset.fillna(dataset.mean(), inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "df_test.insert(0, 'PeerRank', 0)\n",
    "dataset = pd.concat(objs=[df_train, df_test], axis=0)\n",
    "\n",
    "dataset = feature_engineer(dataset)\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = dataset.dtypes[dataset.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "dataset[skewed_feats] = np.log1p(dataset[skewed_feats])\n",
    "dataset = pd.get_dummies(dataset)\n",
    "\n",
    "train_num = len(df_train)\n",
    "df_train = dataset[:train_num]\n",
    "df_test = dataset[train_num:]\n",
    "\n",
    "\n",
    "ytrain = df_train['PeerRank']\n",
    "ytrain = np.log1p(ytrain)\n",
    "\n",
    "df_train.drop(\"PeerRank\", inplace=True, axis=1) \n",
    "df_train.drop(\"period\", inplace = True, axis=1)\n",
    "df_train.drop(\"RankCategory\", axis=1, inplace=True)\n",
    "    \n",
    "df_test.drop(\"period\", inplace = True, axis=1)\n",
    "df_test.drop(\"RankCategory\", axis=1, inplace=True)       \n",
    "df_test.drop('PeerRank', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KerasClassifier and KerasRegressor classes in Keras take an argument build_fn which is the name of the function to call to get your model.\n",
    "\n",
    "You must define a function called whatever you like that defines your model, compiles it and returns it.\n",
    "\n",
    "In the example, below we define a function baseline_model() that create a simple multi-layer neural network for the problem.\n",
    "\n",
    "We pass this function name to the KerasRegressor class by the build_fn argument. We also pass in additional arguments of nb_epoch=150 and batch_size=10. These are automatically bundled up and passed on to the fit() function which is called internally by the KerasClassifier class.\n",
    "\n",
    "In this example, we use the scikit-learn StratifiedKFold to perform 10-fold stratified cross-validation. This is a resampling technique that can provide a robust estimate of the performance of a machine learning model on unseen data.\n",
    "\n",
    "We use the scikit-learn function cross_val_score() to evaluate our model using the cross-validation scheme and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model(dropout_rate=0.2, optimizer='rmsprop', init='glorot_uniform', weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4096, activation='relu', kernel_initializer='uniform', input_shape=(891,), kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4096)              3653632   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,570,625\n",
      "Trainable params: 8,558,465\n",
      "Non-trainable params: 12,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(df_train, ytrain):\n",
    "    # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    numpy.random.seed(seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_train, ytrain, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(\"Train Data:\", X_train.shape)\n",
    "    # evaluate model with standardized dataset\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "    \n",
    "    estimator.fit(X_train.values, y_train.values)\n",
    "    rmse = math.sqrt(mean_squared_error(y_val.values, estimator.predict(X_val.values)))\n",
    "    print(rmse)\n",
    "    \n",
    "    # evaluate using 10-fold cross validation\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    results = cross_val_score(estimator, df_train.values, ytrain.values, cv=kfold)\n",
    "    print(\"mean score:\", results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search Deep Learning Model Parameters\n",
    "\n",
    "The previous example showed how easy it is to wrap your deep learning model from Keras and use it in functions from the scikit-learn library.\n",
    "\n",
    "In this example, we go a step further. The function that we specify to the build_fn argument when creating the KerasRegressor wrapper can take arguments. We can use these arguments to further customize the construction of the model. In addition, we know we can provide arguments to the fit() function.\n",
    "\n",
    "In this example, we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best-estimated performance.\n",
    "\n",
    "The create_model() function is defined to take two arguments optimizer and init, both of which must have default values. This will allow us to evaluate the effect of using different optimization algorithms and weight initialization schemes for our network.\n",
    "\n",
    "After creating our model, we define arrays of values for the parameter we wish to search, specifically:\n",
    "\n",
    "Optimizers for searching different weight values.\n",
    "Initializers for preparing the network weights using different schemes.\n",
    "Epochs for training the model for a different number of exposures to the training dataset.\n",
    "Batches for varying the number of samples before a weight update.\n",
    "The options are specified into a dictionary and passed to the configuration of the GridSearchCV scikit-learn class. This class will evaluate a version of our neural network model for each combination of parameters (2 x 3 x 3 x 3 for the combinations of optimizers, initializations, epochs and batches). Each combination is then evaluated using the default of 3-fold stratified cross validation.\n",
    "\n",
    "That is a lot of models and a lot of computation. This is not a scheme that you want to use lightly because of the time it will take. It may be useful for you to design small experiments with a smaller subset of your data that will complete in a reasonable time. This is reasonable in this case because of the small network and the small dataset (less than 1000 instances and 9 attributes).\n",
    "\n",
    "Finally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take about 5 minutes to complete on your workstation executed on the CPU (rather than CPU). running the example shows the results below.\n",
    "\n",
    "We can see that the grid search discovered that using a uniform initialization scheme, rmsprop optimizer, 150 epochs and a batch size of 5 achieved the best cross-validation score of approximately 75% on this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (4071, 891)\n",
      "58.04536393479158\n",
      "mean score: -3431.64293268\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(df_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridSearch_neural_network(df_train, ytrain):\n",
    "    # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    numpy.random.seed(seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_train, ytrain, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(\"Train Data:\", X_train.shape)\n",
    "    print(\"Train label:\", y_train.shape)\n",
    "    #print(\"Test Data:\", Xtest.shape)\n",
    "    # evaluate model with standardized dataset\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "    \n",
    "    #estimator.fit(X_train.values, y_train.values)\n",
    "    #rmse = math.sqrt(mean_squared_error(y_val.values, estimator.predict(X_val.values)))\n",
    "    #print(rmse)\n",
    " \n",
    "    # grid search epochs, batch size and optimizer\n",
    "    optimizers = ['rmsprop', 'adam']\n",
    "    dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = [5, 10, 20]\n",
    "    weight_constraint = [1, 2, 3, 4, 5]\n",
    "    param_grid = dict(optimizer=optimizers, \n",
    "                      dropout_rate=dropout_rate, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batches, \n",
    "                      weight_constraint=weight_constraint, \n",
    "                      init=init)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=estimator, param_grid=param_grid)\n",
    "    grid_result = grid.fit(X_train.values, y_train.values)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (4071, 891)\n",
      "Train label: (4071,)\n"
     ]
    }
   ],
   "source": [
    "gridSearch_neural_network(df_train, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this post, you discovered how you can wrap your Keras deep learning models and use them in the scikit-learn general machine learning library.\n",
    "\n",
    "You can see that using scikit-learn for standard machine learning operations such as model evaluation and model hyperparameter optimization can save a lot of time over implementing these schemes yourself.\n",
    "\n",
    "Wrapping your model allowed you to leverage powerful tools from scikit-learn to fit your deep learning models into your general machine learning process.\n",
    "\n",
    "Do you have any questions about using Keras models in scikit-learn or about this post? Ask your question in the comments or send me email at GTK@capgroup.com and I will do my best to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
