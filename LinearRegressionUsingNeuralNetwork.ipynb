{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Neural Network written in Keras - A Popular Deep Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras a warpper API that runs on top of Tensorflow or theano is very popular and easy to use. Scikit learn also very popular libraries for machine learning.In this post I will show how to use keras and scikit learn to build neural network architecture in python and develop a regression linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.linear_model.ridge import Ridge\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "from sklearn.svm.classes import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(sklearn.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Keras APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.constraints import maxnorm\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we will solve here is to predict the rank of a product based on the product characteristics or features available.\n",
    "Our objective is to train a regression model in keras to be able to predict rate for new product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def baseline_model_1057(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1058, activation='relu', \n",
    "                    kernel_regularizer = 'l2', \n",
    "                    kernel_initializer = 'normal', \n",
    "                    input_shape=(1057,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(529, activation='relu', \n",
    "                    kernel_regularizer = 'l2',\n",
    "                    kernel_initializer = 'normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer = 'l2', \n",
    "                    kernel_initializer='normal'))\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_data_nn(X_train, y_train):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    # create model\n",
    "    estimator = KerasRegressor(build_fn=baseline_model_1057, epochs=100, batch_size=10, verbose=0)\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    results = cross_val_score(estimator, X_train, y_train, cv=kfold)  \n",
    "    print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_learning_curve(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_and_predict(Xtrain, Xtest):\n",
    "    X = Xtrain\n",
    "    y = X['rank']\n",
    "    X.drop(\"rank\", inplace=True, axis=1) \n",
    "    \n",
    "    null_cols = X.columns[X.isnull().all()]\n",
    "    X.drop(null_cols, inplace=True, axis=1)\n",
    "    nunique = X.apply(pd.Series.nunique)\n",
    "    null_col_uni = nunique[nunique == 1].index\n",
    "    X.drop(null_col_uni, inplace=True, axis=1)\n",
    "    \n",
    "    \n",
    "    Xtest.drop(null_cols, inplace=True, axis=1)   \n",
    "    Xtest.drop(null_col_uni, inplace=True, axis=1)\n",
    "\n",
    "    print('Train size:', X.shape, ' Test size:', Xtest.shape)\n",
    "    \n",
    "    seed = 7\n",
    "    numpy.random.seed(seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    estimator = train_data_nn(X_train, y_train)\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=1, verbose=1) \n",
    "    history = estimator.fit(X_train, y_train, validation_split=0.1, \n",
    "                            epochs=100, batch_size=10, \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=1)\n",
    "    visualize_learning_curve(history)\n",
    "    rmse = math.sqrt(mean_squared_error(y_val.values, estimator.predict(X_val.values)))\n",
    "    print(rmse)\n",
    "    pred = estimator.predict(X_test)\n",
    "    test_df = pd.DataFrame({'y_pred': pred})    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (4524, 1058)\n",
      "Test Data: (3016, 1057)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "train_num = len(df_train)\n",
    "df_test.insert(0, 'rank', 0)\n",
    "dataset = pd.concat(objs=[df_train, df_test], axis=0)\n",
    "dataset = shuffle(dataset)\n",
    "dataset.fillna(0, inplace=True)\n",
    "df_train = dataset[:train_num]\n",
    "df_test = dataset[train_num:]\n",
    "df_test.drop('rank', inplace=True, axis=1)\n",
    "print(\"Train Data:\", df_train.shape)\n",
    "print(\"Test Data:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (4524, 1057)  Test size: (3016, 1057)\n"
     ]
    }
   ],
   "source": [
    "test_df = train_and_predict_new(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = test_df\n",
    "submission.sort_index(inplace=True)\n",
    "submission.loc[submission['y_pred'] < 0, 'y_pred'] = 0\n",
    "submission.loc[submission['y_pred'] > 100, 'y_pred'] = 100\n",
    "submission.to_csv(\"./data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Grid Search Deep Learning Model Parameters\n",
    "\n",
    "The previous example showed how easy it is to wrap your deep learning model from Keras and use it in functions from the scikit-learn library.\n",
    "\n",
    "In this example, we go a step further. The function that we specify to the build_fn argument when creating the KerasRegressor wrapper can take arguments. We can use these arguments to further customize the construction of the model. In addition, we know we can provide arguments to the fit() function.\n",
    "\n",
    "In this example, we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best-estimated performance.\n",
    "\n",
    "The create_model() function is defined to take two arguments optimizer and init, both of which must have default values. This will allow us to evaluate the effect of using different optimization algorithms and weight initialization schemes for our network.\n",
    "\n",
    "After creating our model, we define arrays of values for the parameter we wish to search, specifically:\n",
    "\n",
    "Optimizers for searching different weight values. Initializers for preparing the network weights using different schemes. Epochs for training the model for a different number of exposures to the training dataset. Batches for varying the number of samples before a weight update. The options are specified into a dictionary and passed to the configuration of the GridSearchCV scikit-learn class. This class will evaluate a version of our neural network model for each combination of parameters (2 x 3 x 3 x 3 for the combinations of optimizers, initializations, epochs and batches). Each combination is then evaluated using the default of 3-fold stratified cross validation.\n",
    "\n",
    "That is a lot of models and a lot of computation. This is not a scheme that you want to use lightly because of the time it will take. It may be useful for you to design small experiments with a smaller subset of your data that will complete in a reasonable time. This is reasonable in this case because of the small network and the small dataset (less than 1000 instances and 9 attributes).\n",
    "\n",
    "Finally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters.\n",
    "\n",
    "This might take about 5 minutes to complete on your workstation executed on the CPU (rather than CPU). running the example shows the results below.\n",
    "\n",
    "We can see that the grid search discovered that using a uniform initialization scheme, rmsprop optimizer, 150 epochs and a batch size of 5 achieved the best cross-validation score of approximately 75% on this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridSearch_neural_network(df_train, ytrain):\n",
    "    # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    numpy.random.seed(seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_train, ytrain, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(\"Train Data:\", X_train.shape)\n",
    "    print(\"Train label:\", y_train.shape)\n",
    "    # evaluate model with standardized dataset\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "    \n",
    "    # grid search epochs, batch size and optimizer\n",
    "    optimizers = ['rmsprop', 'adam']\n",
    "    dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = [5, 10, 20]\n",
    "    weight_constraint = [1, 2, 3, 4, 5]\n",
    "    param_grid = dict(optimizer=optimizers, \n",
    "                      dropout_rate=dropout_rate, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batches, \n",
    "                      weight_constraint=weight_constraint, \n",
    "                      init=init)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=estimator, param_grid=param_grid)\n",
    "    grid_result = grid.fit(X_train.values, y_train.values)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, you discovered how you can wrap your Keras deep learning models and use them in the scikit-learn general machine learning library.\n",
    "\n",
    "You can see that using scikit-learn for standard machine learning operations such as model evaluation and model hyperparameter optimization can save a lot of time over implementing these schemes yourself.\n",
    "\n",
    "Wrapping your model allowed you to leverage powerful tools from scikit-learn to fit your deep learning models into your general machine learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
